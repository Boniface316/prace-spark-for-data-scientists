{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# **Exercise 2: Spark SQL**\n",
    "#### This second exercise will introduce database operations with Spark. We will start by introducing Spark SQL, moving onto exploratory analysis on Apache web server logs collected from the http://fileadmin.cs.lth.se website and ending with finding out which course is the most popular in fileadmin.\n",
    "\n",
    "### **The following material will be covered:**\n",
    "#### *Part 1:* Learning Spark SQL\n",
    "#### *Part 2:* Exploratory analysis of fileadmin dataset\n",
    "#### *Part 3:* Visualizing the day and month rhythms\n",
    "#### *Part 4:* Finding the most popular course\n",
    "\n",
    "### During the exercises, the following resources might come in handy:\n",
    "* #### Documentation of the [PySpark API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)\n",
    "* #### Documentation of the [Python API](https://docs.python.org/2.7/)\n",
    "* #### Documentation of the [Spark SQL API](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "* #### Documentation of [Hive SQL](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)\n",
    "\n",
    "### To run code in Jupyter, press: \n",
    "* #### `Ctrl-Enter` to run the code in the currently selected cell\n",
    "* #### `Shift-Enter` to run the code in the currently selected cell and jump to the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SPARK_DRIVER_MEMORY\"] = \"1792M\"\n",
    "os.environ[\"SPARK_OPTS\"] = \"--driver-java-options=-Xms1024M --driver-java-options=-Xmx1536M --driver-java-options=-Dlog4j.logLevel=info\"\n",
    "\n",
    "#from pyspark import SparkContext\n",
    "#from pyspark.sql import SQLContext\n",
    "\n",
    "#sc = SparkContext(master=\"local[*]\")\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Helper: Displays rows from a Spark SQL object as HTML**\n",
    "#### The following code can display arrays of rows or dataframes as a html table. This creates a human-friendly output of our data. It is enough to browse through this code, as it is not important to know it in detail for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "\n",
    "def displayRows(rowDf):\n",
    "    headers = []\n",
    "    rows = []\n",
    "    if(str(type(rowDf)) == \"<class 'pyspark.sql.dataframe.DataFrame'>\"):\n",
    "        rows = rowDf.limit(10000).collect() #Let's limit the output just in case!\n",
    "        if(len(rows) == 10000):\n",
    "            if(rowDf.limit(10001).count() == 10001):\n",
    "                warnings.warn(\"More than 10 000 rows was returned, only showing the first 10 000.\")\n",
    "                \n",
    "        headers = list(rowDf.columns)\n",
    "    else:\n",
    "        rows = rowDf\n",
    "        if(len(rows) > 10000):\n",
    "            warnings.warn(\"Rows has {0} elements, only showing the first 10 000.\".format(len(rows)))\n",
    "            rows = rows[0:10000]\n",
    "            \n",
    "        #Computes the unique set of keys\n",
    "        headers = list(sorted(reduce(lambda x,y: x.union(set(y.asDict().iterkeys())), rows, set())))\n",
    "            \n",
    "    tableHead = [\"<th>{0}</th>\".format(key) for key in headers]\n",
    "    tableBody = [\"<tr>{0}</tr>\".format(\n",
    "                    \"\".join([\"<td>{0}</td>\".format(rowDict.get(header)) \n",
    "                            for rowDict \n",
    "                            in (row.asDict(),) \n",
    "                            for header \n",
    "                            in headers])\n",
    "                    ) for row in rows]\n",
    "    \n",
    "    display(HTML(\n",
    "    u\"\"\"<table>\n",
    "    <thead><tr>{0}</tr></thead>\n",
    "    <tbody>{1}</tbody>\n",
    "    </table>\n",
    "    \"\"\".format(\"\".join(tableHead), \"\".join(tableBody))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Learning Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This part will introduce you the Spark SQL by writing SQL queries.\n",
    "\n",
    "The cell below generates data which you will write queries for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 20 boy and girl names 2014 in random order.\n",
    "names = [\"Caden\", \"Kaylee\", \"Lucas\", \"Ethan\", \"Alexander\", \"Jackson\", \n",
    "         \"Aiden\", \"Madelyn\", \"Michael\", \"Avery\", \"Luke\", \"Isabella\", \n",
    "         \"Chloe\", \"Elijah\", \"Abigail\", \"Madison\", \"Jacob\", \"Zoe\", \"Emily\", \n",
    "         \"Jayden\", \"Liam\", \"Mason\", \"Mia\", \"Sophia\", \"Benjamin\", \"Layla\", \n",
    "         \"Emma\", \"Lily\", \"Charlotte\", \"Caleb\", \"James\", \"Noah\", \"Ella\", \n",
    "         \"Jack\", \"Jayce\", \"Aubrey\", \"Olivia\", \"Harper\", \"Logan\", \"Ava\"]\n",
    "\n",
    "#A-G in phonetic alphabet\n",
    "groups = [\"Alpha\",\"Bravo\", \"Charlie\", \"Delta\", \"Echo\", \"Foxtrot\", \"Golf\"]\n",
    "\n",
    "#Some numeric magic to generate not so uniform random data.\n",
    "tblUserRdd = sc.parallelize(map(lambda i: (i, ((i*104729)^131) % 7, 26500 + ((i*104729)^96587) % 6367), range(1,51)))\n",
    "tblNamesRdd = sc.parallelize(enumerate(names, 1), 4)\n",
    "tblGroupNamesRdd = sc.parallelize(enumerate(groups), 2)\n",
    "\n",
    "#Create dataframes from the RDDs\n",
    "tblNames      = sqlContext.createDataFrame(tblNamesRdd,      [\"userId\", \"name\"])\n",
    "tblUsers      = sqlContext.createDataFrame(tblUserRdd,       [\"id\", \"groupId\", \"salary\"])\n",
    "tblGroupNames = sqlContext.createDataFrame(tblGroupNamesRdd, [\"id\", \"name\"])\n",
    "\n",
    "#Register them for use.\n",
    "sqlContext.registerDataFrameAsTable(tblGroupNames, \"tblGroupNames\")\n",
    "sqlContext.registerDataFrameAsTable(tblUsers, \"tblUsers\")\n",
    "sqlContext.registerDataFrameAsTable(tblNames, \"tblNames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, lets get some basic information about each dataframe\n",
    "\n",
    "Dataframes are structured meaning that types and columns are well-defined; if you have read the data generation cell you might have noticed that the types were not specified. These are inferred by Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframes provide a very handy function called `printSchema()`. As its name implies, it shows the schema of the data, including column names and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- groupId: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tblUsers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is possible to call a number of operations on dataframe, similar to RDDs dataframes have a `count()` action to display the number of rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblUsers.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tblGroupNames.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblGroupNames.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- groupId: long (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tblUsers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tblNames.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next 3 cells will display the content of the dataframe by using the helper function *displayRows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><th>id</th><th>groupId</th><th>salary</th></tr></thead>\n",
       "    <tbody><tr><td>1</td><td>5</td><td>26623</td></tr><tr><td>2</td><td>5</td><td>30452</td></tr><tr><td>3</td><td>5</td><td>30462</td></tr><tr><td>4</td><td>6</td><td>27932</td></tr><tr><td>5</td><td>6</td><td>26973</td></tr><tr><td>6</td><td>2</td><td>32796</td></tr><tr><td>7</td><td>2</td><td>29202</td></tr><tr><td>8</td><td>3</td><td>32531</td></tr><tr><td>9</td><td>3</td><td>28969</td></tr><tr><td>10</td><td>3</td><td>29034</td></tr><tr><td>11</td><td>0</td><td>27978</td></tr><tr><td>12</td><td>1</td><td>30759</td></tr><tr><td>13</td><td>1</td><td>31825</td></tr><tr><td>14</td><td>1</td><td>28231</td></tr><tr><td>15</td><td>1</td><td>30599</td></tr><tr><td>16</td><td>5</td><td>29023</td></tr><tr><td>17</td><td>5</td><td>32820</td></tr><tr><td>18</td><td>5</td><td>32862</td></tr><tr><td>19</td><td>5</td><td>30292</td></tr><tr><td>20</td><td>6</td><td>29373</td></tr><tr><td>21</td><td>3</td><td>28285</td></tr><tr><td>22</td><td>3</td><td>28018</td></tr><tr><td>23</td><td>3</td><td>28156</td></tr><tr><td>24</td><td>4</td><td>31849</td></tr><tr><td>25</td><td>4</td><td>30922</td></tr><tr><td>26</td><td>0</td><td>30346</td></tr><tr><td>27</td><td>0</td><td>26784</td></tr><tr><td>28</td><td>1</td><td>27954</td></tr><tr><td>29</td><td>1</td><td>30631</td></tr><tr><td>30</td><td>1</td><td>26600</td></tr><tr><td>31</td><td>5</td><td>31911</td></tr><tr><td>32</td><td>6</td><td>28341</td></tr><tr><td>33</td><td>6</td><td>29503</td></tr><tr><td>34</td><td>6</td><td>32180</td></tr><tr><td>35</td><td>6</td><td>32245</td></tr><tr><td>36</td><td>3</td><td>30685</td></tr><tr><td>37</td><td>3</td><td>30386</td></tr><tr><td>38</td><td>3</td><td>30556</td></tr><tr><td>39</td><td>3</td><td>27858</td></tr><tr><td>40</td><td>4</td><td>26923</td></tr><tr><td>41</td><td>1</td><td>32234</td></tr><tr><td>42</td><td>1</td><td>29664</td></tr><tr><td>43</td><td>1</td><td>29834</td></tr><tr><td>44</td><td>2</td><td>29415</td></tr><tr><td>45</td><td>2</td><td>28488</td></tr><tr><td>46</td><td>2</td><td>27784</td></tr><tr><td>47</td><td>5</td><td>30717</td></tr><tr><td>48</td><td>6</td><td>31903</td></tr><tr><td>49</td><td>6</td><td>28181</td></tr><tr><td>50</td><td>6</td><td>28278</td></tr></tbody>\n",
       "    </table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayRows(tblUsers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><th>userId</th><th>name</th></tr></thead>\n",
       "    <tbody><tr><td>1</td><td>Caden</td></tr><tr><td>2</td><td>Kaylee</td></tr><tr><td>3</td><td>Lucas</td></tr><tr><td>4</td><td>Ethan</td></tr><tr><td>5</td><td>Alexander</td></tr><tr><td>6</td><td>Jackson</td></tr><tr><td>7</td><td>Aiden</td></tr><tr><td>8</td><td>Madelyn</td></tr><tr><td>9</td><td>Michael</td></tr><tr><td>10</td><td>Avery</td></tr><tr><td>11</td><td>Luke</td></tr><tr><td>12</td><td>Isabella</td></tr><tr><td>13</td><td>Chloe</td></tr><tr><td>14</td><td>Elijah</td></tr><tr><td>15</td><td>Abigail</td></tr><tr><td>16</td><td>Madison</td></tr><tr><td>17</td><td>Jacob</td></tr><tr><td>18</td><td>Zoe</td></tr><tr><td>19</td><td>Emily</td></tr><tr><td>20</td><td>Jayden</td></tr><tr><td>21</td><td>Liam</td></tr><tr><td>22</td><td>Mason</td></tr><tr><td>23</td><td>Mia</td></tr><tr><td>24</td><td>Sophia</td></tr><tr><td>25</td><td>Benjamin</td></tr><tr><td>26</td><td>Layla</td></tr><tr><td>27</td><td>Emma</td></tr><tr><td>28</td><td>Lily</td></tr><tr><td>29</td><td>Charlotte</td></tr><tr><td>30</td><td>Caleb</td></tr><tr><td>31</td><td>James</td></tr><tr><td>32</td><td>Noah</td></tr><tr><td>33</td><td>Ella</td></tr><tr><td>34</td><td>Jack</td></tr><tr><td>35</td><td>Jayce</td></tr><tr><td>36</td><td>Aubrey</td></tr><tr><td>37</td><td>Olivia</td></tr><tr><td>38</td><td>Harper</td></tr><tr><td>39</td><td>Logan</td></tr><tr><td>40</td><td>Ava</td></tr></tbody>\n",
       "    </table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayRows(tblNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead><tr><th>id</th><th>name</th></tr></thead>\n",
       "    <tbody><tr><td>0</td><td>Alpha</td></tr><tr><td>1</td><td>Bravo</td></tr><tr><td>2</td><td>Charlie</td></tr><tr><td>3</td><td>Delta</td></tr><tr><td>4</td><td>Echo</td></tr><tr><td>5</td><td>Foxtrot</td></tr><tr><td>6</td><td>Golf</td></tr></tbody>\n",
       "    </table>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayRows(tblGroupNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is a basic function for displaying the contents of an Dataframe by using *show()*\n",
    "However, the output is limited and gives a limited view of a long column. It is useful for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n",
      "| id|groupId|salary|\n",
      "+---+-------+------+\n",
      "|  1|      5| 26623|\n",
      "|  2|      5| 30452|\n",
      "|  3|      5| 30462|\n",
      "|  4|      6| 27932|\n",
      "|  5|      6| 26973|\n",
      "|  6|      2| 32796|\n",
      "|  7|      2| 29202|\n",
      "|  8|      3| 32531|\n",
      "|  9|      3| 28969|\n",
      "| 10|      3| 29034|\n",
      "| 11|      0| 27978|\n",
      "| 12|      1| 30759|\n",
      "| 13|      1| 31825|\n",
      "| 14|      1| 28231|\n",
      "| 15|      1| 30599|\n",
      "| 16|      5| 29023|\n",
      "| 17|      5| 32820|\n",
      "| 18|      5| 32862|\n",
      "| 19|      5| 30292|\n",
      "| 20|      6| 29373|\n",
      "+---+-------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tblUsers.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, the first query you will write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a) Write a query that selects all user ids in the group with id 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 2\n",
    "assert a % 2 == 0, \"value was odd, should be even\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "q1a = sqlContext.sql(\"\"\"\n",
    "SELECT id \n",
    "FROM tblUsers \n",
    "WHERE <FILL IN>\n",
    "\"\"\")\n",
    "\n",
    "displayRows(q1a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert set(map(lambda row: row.id, q1a.collect())) == set([11,26,27])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b) Write a query that finds the min and max userId grouped by groupId\n",
    "\n",
    "The result should have the following columns:\n",
    "\n",
    "1. minUserId: The min user id per group\n",
    "2. maxUserId: The max user id per group\n",
    "2. groupId: The group id\n",
    "\n",
    "**Hint:** Use GROUP BY, MIN, MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1b = sqlContext.sql(\"\"\"\n",
    "SELECT \n",
    "    <FILL IN> AS minUserId, \n",
    "    <FILL IN> AS maxUserId,\n",
    "    <FILL IN> \n",
    "FROM tblUsers \n",
    "<FILL IN>\n",
    "\"\"\")\n",
    "\n",
    "displayRows(q1b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minIds = {0: 11,\n",
    " 1: 12,\n",
    " 2: 6,\n",
    " 3: 8,\n",
    " 4: 24,\n",
    " 5: 1,\n",
    " 6: 4}\n",
    "\n",
    "maxIds = {0: 27,\n",
    " 1: 43,\n",
    " 2: 46,\n",
    " 3: 39,\n",
    " 4: 40,\n",
    " 5: 47,\n",
    " 6: 50}\n",
    "\n",
    "assert all(map(lambda row: minIds[row.groupId] == row.minUserId, q1b.collect()))\n",
    "assert all(map(lambda row: maxIds[row.groupId] == row.maxUserId, q1b.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c) Compute the global average salary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you do not specify any group by columns and use aggregating functions such as **AVG**(column) then the aggregation will be performed over the entire result and return a single row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avgSalary = sqlContext.sql(\"\"\"\n",
    "SELECT <FILL IN> AS avgSalary \n",
    "FROM tblUsers\n",
    "\"\"\").collect()[0].avgSalary\n",
    "\n",
    "avgSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert avgSalary == 29707.34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d) Aggregate salaries per group\n",
    "\n",
    "Group per groupId and compute the minimum, average, maximum salary and sort by average salary descending.\n",
    "\n",
    "**Hint:** Use MIN, AVG, MAX, GROUP BY, you can sort by computed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1d = sqlContext.sql(\"\"\"\n",
    "SELECT \n",
    "    groupId,\n",
    "    COUNT(id) AS NumUsers,\n",
    "    <FILL IN> AS MinSalary, \n",
    "    <FILL IN> AS AvgSalary,\n",
    "    <FILL IN> AS MaxSalary,\n",
    "    AVG(salary) - {} AS GlobalAvgDelta\n",
    "FROM tblUsers\n",
    "<FILL IN>\n",
    "ORDER BY <FILL IN>\n",
    "\"\"\".format(avgSalary))\n",
    "\n",
    "displayRows(q1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = [\n",
    "    (5, 26623, 30573, 32862), \n",
    "    (4, 26923, 29898, 31849),\n",
    "    (1, 26600, 29833, 32234),\n",
    "    (2, 27784, 29537, 32796),\n",
    "    (6, 26973, 29490, 32245),\n",
    "    (3, 27858, 29447, 32531),\n",
    "    (0, 26784, 28369, 30346)\n",
    "]\n",
    "\n",
    "q1dresult = q1d.collect()\n",
    "assert len(q1dresult) == 7\n",
    "assert map(lambda i: q1dresult[i].groupId == groups[i][0], xrange(0,len(q1dresult)-1)), \"GroupID column does not match.\"\n",
    "assert map(lambda i: q1dresult[i].MinSalary == groups[i][1], xrange(0,len(q1dresult)-1)), \"MinSalary column does not match.\"\n",
    "assert map(lambda i: int(q1dresult[i].AvgSalary) == groups[i][2], xrange(0,len(q1dresult)-1)), \"AvgSalary column does not match.\"\n",
    "assert map(lambda i: q1dresult[i].MaxSalary == groups[i][3], xrange(0,len(q1dresult)-1)), \"MaxSalary column does not match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 2: Exploratory analysis of fileadmin dataset**\n",
    "#### In this part, we will explore a parsed apache log from fileadmin\n",
    "\n",
    "#### We start by loading our Apache log into a dataframe.\n",
    "As before, we will use a SQLContext provided in the variable `sqlContext`, calling the `read.load()` function and supplying the path to the Apache log. Following this step, we register the dataframe as the table `fadmLog` using `registerDataFrameAsTable()`. This allows us to access the dataframe as a table using the name `fadmLog` in any subsequent SQL call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first line \"matplotlib inline\" is a directive that tells jupyter notebook to render all plots inline, i.e. as images and displaying them here. This is not valid python code and will only work in jupyter notebooks or ipython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "from os.path import abspath\n",
    "\n",
    "fadmLog = sqlContext.read.parquet(\"file:\" + abspath(\"../data/fileadmin-logs.parquet/\")).persist()\n",
    "fadmLog.printSchema()\n",
    "sqlContext.registerDataFrameAsTable(fadmLog, \"fadmLog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fadmLog.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's take a look at 10 rows from the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "displayRows(fadmLog.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.a) Determine number of entries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fadmLog.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.b) Determine time range**\n",
    "#### We see that our log contains more than 19 million entries.\n",
    "\n",
    "#### Next, we wish to determine the date of the first and last entry in the log.\n",
    "\n",
    "#### Complete the SQL statement.\n",
    "Select the oldest and the newest entry from the log using the `MIN()`, and `MAX()` statements. Use the `from_unixtime()` statement to convert the timestamps into a human-friendly format.\n",
    "\n",
    "We expect a dataframe containing a single row with two columns, the **first(oldest) entry** and the **last(latest) entry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "firstLastDf = sqlContext.sql(\"\"\"\n",
    "    <FILL IN>\n",
    "  \"\"\").cache()\n",
    "\n",
    "displayRows(firstLastDf.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the first log entry\n",
    "assert firstLastDf.collect()[0][0] == u'2011-06-21 23:44:15', \"Wrong date of the first log entry!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.c) Determine the number of requests per year**\n",
    "#### Which year did our website receive most requests?\n",
    "Show each year and the total number of requests that year in ascending order by year.\n",
    "\n",
    "#### Complete the SQL statement below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "numberOfRequests = sqlContext.sql(\"\"\" \n",
    "                                     <FILL IN>\n",
    "                                     \"\"\").cache()\n",
    "\n",
    "displayRows(numberOfRequests.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the yearly summary\n",
    "assert len(numberOfRequests.collect()) == 5, \"Expected to see data from 5 years!\"\n",
    "assert numberOfRequests.collect()[3][1] == 5890942, \"Wrong number of requests in year 2014!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.d) Determine the number of requests per year/month**\n",
    "#### Do the number requests peak at certain month?\n",
    "Repeat the previous exercise, displaying the month in addition to the year.\n",
    "\n",
    "Show each year, month and the total number of requests that year in ascending order by year and month.\n",
    "\n",
    "#### Complete the SQL statement below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "numberOfRequestsYearMonth = sqlContext.sql(\"\"\"\n",
    "                                              <FILL IN>\n",
    "                                              \"\"\").cache()\n",
    "\n",
    "displayRows(numberOfRequestsYearMonth.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the yearly summary\n",
    "assert len(numberOfRequestsYearMonth.collect()) == 50, \"Expected to see data from 50 months!\"\n",
    "assert numberOfRequestsYearMonth.collect()[17][2] == 1225716, \"Wrong number of requests in November, 2012!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.e) Determine the all time high 10 days of requests**\n",
    "#### Find the 10 days with the highest number of requests. We expect a dataframe with 10 rows.\n",
    "#### The columns must contain a date and the number of request for that date.\n",
    "Sort the dataframe by the number of requests in descending order.\n",
    "\n",
    "#### Complete the SQL statement below, use `GROUP BY`, `ORDER BY`, and `LIMIT` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "topTenRequests = sqlContext.sql(\"\"\"\n",
    "SELECT\n",
    "    PRINTF(\"%04d-%02d-%02d\", year, month, day) AS date\n",
    "    <FILL IN>\n",
    "                                    \"\"\")\n",
    "\n",
    "displayRows(topTenRequests.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the summary\n",
    "assert len(topTenRequests.collect()) == 10, \"Expected to see data from 10 dates!\"\n",
    "assert topTenRequests.collect()[0][1] == 924902, \"Wrong number of requests, 2012-11-21!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.f) Determine the top 10 requests by source under a day**\n",
    "\n",
    "#### Which sources are responsible for the most number of requests?\n",
    "Find the top 10 sources and dates having the highest number of requests.\n",
    "\n",
    "#### We expect a dataframe with 10 rows.\n",
    "The columns must contain a source, a date and the number of request for that source and date. Sort the dataframe by the number of requests in descending order. \n",
    "\n",
    "#### Complete the SQL statement below.\n",
    "\n",
    "**Hints:** use `GROUP BY`, `ORDER BY`, and `LIMIT` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "topTenRequestSources = sqlContext.sql(\"\"\"\n",
    "                                            <FILL IN>\n",
    "                                            \"\"\").cache()\n",
    "\n",
    "displayRows(topTenRequestSources.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the summary\n",
    "assert len(topTenRequestSources.collect()) == 10, \"Expected to see data from 10 dates!\"\n",
    "assert topTenRequestSources.collect()[0].source == u'130.235.16.54', \"Wrong top source!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.g) Determine the top 100 requests of a single resource by source during single a day**\n",
    "\n",
    "#### Which resource is most requested?\n",
    "Find the top 100 resources having the highest number of requests.\n",
    "\n",
    "#### We expect a dataframe with 100 rows.\n",
    "The columns must contain a resource, a source, a date and the number of request for that resource, source and date. Sort the dataframe by the number of requests in descending order. \n",
    "\n",
    "#### Complete the SQL statement below.\n",
    "\n",
    "**Hints:** use `GROUP BY`, `ORDER BY`, and `LIMIT` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "requestsTopHundred = sqlContext.sql(\"\"\"\n",
    "                                        <FILL IN>\n",
    "                                        \"\"\").cache()\n",
    "\n",
    "displayRows(requestsTopHundred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the summary\n",
    "assert len(requestsTopHundred.collect()) == 100, \"Expected to see data from 100 dates!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.h) Find crawlers by behavior**\n",
    "#### Web crawlers usually make a lot of requests to different unique resources. By listing the sources with the highest unique number of resource accessed together with the number of resources requested, we can start to filter out web crawlers.\n",
    "Find the ten most likely web crawlers, display source, number of distinct resources accessed by that source together with number of request.\n",
    "\n",
    "Complete SQL statement below, using the `COUNT(DISTINCT *column*)`, `COUNT`, `GROUP BY`, `ORDER BY`, and `LIMIT` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "crawlers = sqlContext.sql(\"\"\"\n",
    "                                <FILL IN>\n",
    "                                \"\"\").cache()\n",
    "\n",
    "displayRows(crawlers.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the summary\n",
    "assert len(crawlers.collect()) == 10, \"Expected to see data from 10 sources!\"\n",
    "assert crawlers.collect()[0][0] == u'66.249.78.63', \"Expected to see a web crawler from 66.249.78.63 (GoogleBot)!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3: Visualizing the day and month rhythms**\n",
    "#### In this part we will aggregate the number of requests made per hour and weekday and then visualize it.\n",
    "#### A problem with our log is that a few sources are generating a lot of requests. These have the potential of polluting the aggregated statistics. The goal is to find them and eventually filter them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.a) Find the top 10 sources that generates most traffic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expected output:\n",
    "\n",
    "* source - the source that made the requests\n",
    "* numRequests - count of requests per group\n",
    "\n",
    "#### Complete the SQL statement below, use COUNT, GROUP BY, ORDER BY, LIMIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "topRequesters = sqlContext.sql(\"\"\"\n",
    "SELECT <FILL IN>\n",
    "FROM fadmLog\n",
    "GROUP BY <FILL IN>\n",
    "ORDER BY <FILL IN> DESC\n",
    "LIMIT <FILL IN>\n",
    "\"\"\").persist()\n",
    "topRequesters.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.b) Aggregate by hour and weekday the number of requests, filter out the top 10**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we need a User Defined Function (UDF) that expects three columns (year, month, day) and then gives a number of which weekday it is.\n",
    "\n",
    "In newer version Spark, this function will be provided but pre 1.5.0 it did not exist, and this will serve to show how to implement functionality you need when it does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "def getWeekDay(year, month, day):\n",
    "    return datetime.datetime(year,month,day).weekday()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Register the function as getWeekDay and specify that it has an expected output type of Integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerFunction(\"getWeekDay\", getWeekDay, IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondarily, we need an efficient way of filtering out the top 10 requesters\n",
    "#### We could use this method (registering topRequester DataFrame as a table):\n",
    "\n",
    "```sql\n",
    "SELECT source FROM fadmLog \n",
    "LEFT JOIN topRequester ON topRequester.source = fadmLog.source\n",
    "WHERE topRequester.source IS NULL\n",
    "```\n",
    "\n",
    "#### However, this is a horribly inefficient method due to the use of a shuffling for filtering out just 10 items.\n",
    "#### The solution: Use a UDF that checks against a set in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topRequestersSources = topRequesters.map(lambda row: row.source).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topRequestersBroadcast = sc.broadcast(set(topRequestersSources))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def notTopRequester(source):\n",
    "    return not source in topRequestersBroadcast.value #Returns True if source not in the set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "sqlContext.registerFunction(\"notTopRequester\", <FILL IN>, <FILL IN>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, we are ready to wrap up the entire solution from the individual bits.\n",
    "\n",
    "#### Goal: Count the number of requests per hour and weekday from all requests where the source is not one of the top 10 requesters, and finally, sort by weekday, hour.\n",
    "\n",
    "The expected output columns are:\n",
    "\n",
    "1. **hour: Integer** - the hour of the day, can only assume values between 0-23\n",
    "2. **weekday: Integer** - the index of a weekday, can only assume values between 0-6\n",
    "3. **numRequests: Integer** - the number of requests in the group (hour, weekday)\n",
    "\n",
    "#### Hint: A UDF can be used in WHERE clauses and GROUP BY clauses. You can sort by computed/projected columns.\n",
    "#### The WHERE clause expects only a function that returns True or False\n",
    "\n",
    "* Use getWeekDay to get the weekday from year, month, day.\n",
    "* Use notTopRequester to find out which sources that should be selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "hourWeekdayData = sqlContext.sql(\"\"\"\n",
    "SELECT <FILL IN>\n",
    "FROM <FILL IN>\n",
    "WHERE <FILL IN>\n",
    "GROUP BY <FILL IN>\n",
    "ORDER BY <FILL IN>\n",
    "\"\"\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the aggregated data\n",
    "def toHours(row):\n",
    "    return (row.weekday * 24) + row.hour\n",
    "\n",
    "assert (\n",
    "    len(hourWeekdayData) == 168 and \n",
    "    all(toHours(hourWeekdayData[i]) < toHours(hourWeekdayData[i+1]) for i in xrange(0,len(hourWeekdayData)-1)) and\n",
    "    reduce(lambda x,y: x+y, map(lambda row: row.numRequests, hourWeekdayData),0) == 16182240 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.c) Visualize the result of the previous aggregation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Visualization code\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tcks\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "#Set the size of the figure\n",
    "fig.set_size_inches(16,8)\n",
    "weekdays = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "\n",
    "#Convert aggregated data to plotter\n",
    "dateArray = map(lambda row: weekdays[row.weekday] + \"@\" + str(row.hour), hourWeekdayData)\n",
    "requestArray = map(lambda row: row.numRequests, hourWeekdayData)\n",
    "\n",
    "#A little hack to get labels per hour\n",
    "x = np.arange(len(requestArray))+0.5\n",
    "ax.plot(np.linspace(0,7*24-1,7*24),requestArray)\n",
    "ax.set_xlim(0, 24*7)\n",
    "\n",
    "ax.set_xticks( [pos for pos in xrange(0,24*7,6)] )\n",
    "ax.set_xticklabels( [\"{0}@{1}\".format(weekdays[int(x / 24)], x%24) for x in xrange(0,24*7,6) ], rotation=90 ) ;\n",
    "\n",
    "ax.xaxis.set_major_locator(tcks.IndexLocator(6, 0))\n",
    "#ax.xaxis.set_major_formatter(tcks.FuncFormatter(lambda x, pos: ))\n",
    "ax.xaxis.set_minor_locator(tcks.IndexLocator(1, 0))\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Part 4: Finding the most popular course**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We assume that all courses are located at the resource path: /cs/Education/{course}/... and that course codes are letters followed by numbers\n",
    "\n",
    "##### Your task is to compute the number of requests per course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import regex as re\n",
    "p = re.compile(ur'^\\/cs\\/Education\\/([A-Za-z]+[0-9]+)\\/.+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We have provided a function that extracts the course name and handles errors cases gracefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractCourse(text):\n",
    "    if(text != None):\n",
    "        match = p.match(text)\n",
    "        return match.group(1).upper() if match != None else \"\"\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.a) Register extractCourse as a UDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "sqlContext.<FILL IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The general idea here is to show a way of splitting up a problem into 2 queries instead of an all in one. However, the execution will be faster or similar in performance.\n",
    "\n",
    "### **4.b) Select requests and project them into source and course code.**\n",
    "\n",
    "Only select the requests where the source is not a top requester\n",
    "\n",
    "Expected columns:\n",
    "\n",
    "1. **source: String** - The source\n",
    "2. **course: String** - The course code as given by extractCourse\n",
    "\n",
    "#### Hint: Use the UDFs: extractCourse and notTopRequester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rawCourseRequests = sqlContext.sql(\"\"\"\n",
    "    <FILL IN>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test if the filtering works\n",
    "assert set(rawCourseRequests.columns) == set([\"source\", \"course\"])\n",
    "assert rawCourseRequests.count() == 16182240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.c) Register the intermediary table as rawCourseRequests**\n",
    "\n",
    "Now we have an intermediary table that contains rows of requests with course code and source. To make additional queries on this dataframe we can register it as an intermediary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext.registerDataFrameAsTable(rawCourseRequests, \"rawCourseRequests\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.d) Aggregate from rawCourseRequests and filter out invalid courses (e.g. those with \"\" names)**\n",
    "\n",
    "Expected columns are:\n",
    "\n",
    "1. **course : String** - extracted from resource column\n",
    "2. **numRequests** - The count of requests per course\n",
    "3. **numDistinctRequests** - The count of distinct requests per source column\n",
    "\n",
    "Make numDistinctRequests ordered by highest value first.\n",
    "\n",
    "#### Hint: use COUNT, COUNT(DISTINCT), GROUP BY, ORDER BY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace <FILL IN> with the proper code\n",
    "courseStatistics = sqlContext.sql(\"\"\"\n",
    "    <FILL IN>\n",
    "    WHERE course <> \"\"\n",
    "    <FILL IN>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "courseStatistics = courseStatistics.collect()\n",
    "courseStatistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test the aggregated data\n",
    "assert len(filter(lambda row: row.course == \"\", courseStatistics)) == 0\n",
    "assert reduce(lambda x,y: x+y, map(lambda row: row.numRequests, courseStatistics), 0) == 6222642 \n",
    "assert reduce(lambda x,y: x+y, map(lambda row: row.numDistinctRequests, courseStatistics), 0) == 583381 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
